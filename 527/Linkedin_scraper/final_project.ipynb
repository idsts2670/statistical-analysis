{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "## need to install selenium==4.2.0 to run linkedin_scraper (https://github.com/joeyism/linkedin_scraper/issues/129)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from linkedin_scraper import Person, actions, Company\n",
    "import re as re\n",
    "import time\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/satoshiido/Documents/chromedriver_mac_arm64/chromedriver\n",
      "idsts2670@gmail.com\n",
      "Ss01182389!\n"
     ]
    }
   ],
   "source": [
    "# The location of the web driver in your system, a username, and a password to log in with\n",
    "## https://www.linkedin.com/\n",
    "## first get the chrome webdriver on your local: https://bit.ly/3FHqvF7\n",
    "## my webdriver path: /Users/satoshiido/Documents/chromedriver_mac_arm64/chromedriver\n",
    "PATH = input(\"Enter the Webdriver path: \")\n",
    "## input your linkedin account username or email address\n",
    "USERNAME = input(\"Enter the username: \")\n",
    "## input your linkedin account password\n",
    "PASSWORD = input(\"Enter the password: \")\n",
    "print(PATH)\n",
    "print(USERNAME)\n",
    "print(PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://pypi.org/project/linkedin-scraper/\n",
    "# pip3 install --user linkedin_scraper\n",
    "# set your chromedriver location: export CHROMEDRIVER=~/chromedriver on terminal\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change the code for old ver of selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j0/tkdlg4n54w74ht8tklyh79640000gn/T/ipykernel_28321/1811885428.py:4: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(PATH)\n"
     ]
    }
   ],
   "source": [
    "# web documents of WebDriver for Chrome: https://chromedriver.chromium.org/getting-started\n",
    "# how to include the ChromeDriver location in Mac OS System PATH: https://bit.ly/3lvBnz3\n",
    "# when you get error: \"“chromedriver” cannot be opened because the developer cannot be verified\": https://bit.ly/3lqukb1\n",
    "driver = webdriver.Chrome(PATH)\n",
    "driver.get(\"https://www.linkedin.com/uas/login\")\n",
    "time.sleep(3)\n",
    "\n",
    "# tell the driver to login with the credentials provided above\n",
    "email = driver.find_element(By.ID, 'username')\n",
    "email.send_keys(USERNAME)\n",
    "password = driver.find_element(By.ID, 'password')\n",
    "password.send_keys(PASSWORD)\n",
    "time.sleep(3)\n",
    "password.send_keys(Keys.RETURN)\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/in/idsts2670/\n",
    "linkedin_page = input(\"Enter the Company or User Linkedin URL: \")\n",
    "# if its company\n",
    "company_name = linkedin_page[33:-1]\n",
    "# if its user\n",
    "user_name = linkedin_page[28:-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scrape the employees from the company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<selenium.webdriver.remote.webelement.WebElement (session=\"ed940833c04cdb1ef2c55c4f8f50911d\", element=\"278b0511-eadc-4b27-b4bd-d05f9babd257\")>\n"
     ]
    }
   ],
   "source": [
    "# webdriver\n",
    "company = Company(\"https://www.linkedin.com/company/tesla-motors/\", driver = driver, get_employees=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"Tesla\", \"about_us\": \"Our mission is to accelerate the world\\u2019s transition to sustainable energy. With global temperatures rising, the faster we free ourselves from fossil fuel reliance and achieve a zero-emission future, the better. \\n \\nIn pursuit of this mission, we make electric vehicles that are not just great EVs, but the best cars, period. We also produce and install infinitely scalable clean energy generation and storage products that help our customers further decrease their environmental impact. When it comes to achieving our goals, we pride ourselves in accomplishing what others deem impossible.\\n \\nWe are opening new factories and increasing our output everyday \\u2013 join us in building a sustainable future.\", \"specialties\": \"Innovation in electric cars and clean energy products\", \"website\": \"https://www.tesla.com/careers\", \"industry\": \"Motor Vehicle Manufacturing\", \"company_type\": \"Tesla\", \"headquarters\": \"Austin, Texas\", \"company_size\": \"10,001+ employees\", \"founded\": \"2003\", \"affiliated_companies\": [], \"employees\": [null, null, null], \"headcount\": null}\n"
     ]
    }
   ],
   "source": [
    "print(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://christophegaron.com/articles/mind/automation/scraping-linkedin-posts-with-selenium-and-beautiful-soup/\n",
    "## for company profile -> driver.get(page + 'posts/')\n",
    "## for personal profile -> driver.get(page + 'recent-activity/')\n",
    "driver.get(linkedin_page)\n",
    "SCROLL_PAUSE_TIME = 1.5\n",
    "# get scroll height\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    # scroll down to bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # wait to load page\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "    # calculate new scroll height and compare with last scroll height\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect the source code and then start looking for the page elements that we want to access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Data Analyst\\nData Analyst\\nオートロ株式会社 · Internship\\nオートロ株式会社 · Internship\\nMay 2020 - Mar 2021 · 11 mos\\nMay 2020 - Mar 2021 · 11 mos\\nTokyo, Japan · On-site\\nTokyo, Japan · On-site\\n・Customer analysis to increase closing rate and productivity in the sales team\\n・SaaS analysis to decrease company’s Churn rate and to increase the probability of contract renewal.\\n・Customer analysis to increase closing rate and productivity in the sales team ・SaaS analysis to decrease company’s Churn rate and to increase the probability of contract renewal.\\nSkills: Problem Solving · Information Technology · Business Consulting · Statistics · Business Analysis · Statistical Data Analysis · Python (Programming Language) · SQL · R (Programming Language)\\nProblem Solving · Information Technology · Business Consulting · Statistics · Business Analysis · Statistical Data Analysis · Python (Programming Language) · SQL · R (Programming Language)\\nWhat is AUTORO | Cloud-based RPA \"AUTORO\"\\nWhat is AUTORO | Cloud-based RPA \"AUTORO\"', 'Jaihyun Kee\\nJaihyun Kee\\nComputer/Data Science at Purdue University\\nComputer/Data Science at Purdue University\\nConnect', 'Shicheng F.\\nShicheng F.\\nIncoming Software Engineer at Amazon\\nIncoming Software Engineer at Amazon\\nConnect', 'Data Analyst\\nData Analyst\\n株式会社大黒屋 · Internship\\n株式会社大黒屋 · Internship\\nMay 2019 - May 2020 · 1 yr 1 mo\\nMay 2019 - May 2020 · 1 yr 1 mo\\nShinagawa-ku, Tokyo, Japan\\nShinagawa-ku, Tokyo, Japan\\n・Reduction of the cost for new customer acquisition and replacement from outsourcing to in-house marketing\\n・Reduction of the cost for new customer acquisition and replacement from outsourcing to in-house marketing\\nSkills: Problem Solving · Information Technology · Digital Transformation · Business Consulting · Statistics · Salesforce Sales Cloud · Statistical Data Analysis · Google Analytics · Python (Programming Language) · Analytical Skills', 'Data Analyst\\nData Analyst\\nSmartDrive co,ltd · Internship\\nSmartDrive co,ltd · Internship\\nApr 2021 - Jul 2022 · 1 yr 4 mos\\nApr 2021 - Jul 2022 · 1 yr 4 mos\\nTokyo, Japan\\nTokyo, Japan\\n・Development of new features on the company’s products for cross-selling\\n・Cohort analysis to find key elements to reduce Churn and to increase LTV\\n・Development of new features on the company’s products for cross-selling ・Cohort analysis to find key elements to reduce Churn and to increase LTV\\nSkills: Problem Solving · Information Technology · Statistics · Statistical Data Analysis · Process Improvement · Python (Programming Language) · SQL · Looker (Software)\\nProblem Solving · Information Technology · Statistics · Statistical Data Analysis · Process Improvement · Python (Programming Language) · SQL · Looker (Software)', 'Sophia University\\nSophia University\\nBachelor of managemnt, Business Administration and Management, General\\nBachelor of managemnt, Business Administration and Management, General\\nApr 2017 - Mar 2022\\nApr 2017 - Mar 2022\\n・Sophia University is one of the top three private universities in Japan.\\n・Concentrations: Strategic Management, Marketing, Econometrics\\n・Sophia University is one of the top three private universities in Japan. ・Concentrations: Strategic Management, Marketing, Econometrics', 'Statistics Without Borders\\nStatistics Without Borders\\n13,909 members\\n13,909 members\\nJoin', 'Purdue University\\nPurdue University\\nMaster of Science - MS, Applied Statistics\\nMaster of Science - MS, Applied Statistics\\nAug 2022 - Aug 2024\\nAug 2022 - Aug 2024', 'My network\\nMy network\\nSee and manage your connections and interests.\\nSee and manage your connections and interests.', 'Data Visualization\\nData Visualization\\n181,892 followers\\n181,892 followers\\nFollowing', 'Eiken Grade-1\\nEiken Grade-1\\nEiken Foundation of Japan\\nEiken Foundation of Japan\\nIssued Nov 2019\\nIssued Nov 2019\\nShow credential', \"81 profile views\\n81 profile views\\nDiscover who's viewed your profile.\\nDiscover who's viewed your profile.\", 'Chi-Wei Lien\\nChi-Wei Lien\\nPurdue CS Major from Taiwan | Upcoming Software Engineer Intern at 120Water\\nConnect', 'Celina Park\\nCelina Park\\n· 1st\\nFirst degree connection\\n--\\n--\\nMessage', 'Anushka Shah\\nAnushka Shah\\n· 1st\\nFirst degree connection\\nStatistics & CS Grad Student at Purdue\\nStatistics & CS Grad Student at Purdue\\nMessage', 'Information Technology\\nInformation Technology\\n3 experiences across SmartDrive co,ltd and 2 other companies\\n3 experiences across SmartDrive co,ltd and 2 other companies', 'Charlie Brobst\\nCharlie Brobst\\n· 1st\\nFirst degree connection\\nMS CS/Statistics Student at Purdue University\\nMS CS/Statistics Student at Purdue University\\nMessage', 'Heekyung Ahn\\nHeekyung Ahn\\n· 1st\\nFirst degree connection\\nPh.D. Candidate at Purdue\\nPh.D. Candidate at Purdue\\nMessage', 'Linked:Seattle\\nLinked:Seattle\\n63,506 members\\n63,506 members\\nJoin', 'Problem Solving\\nProblem Solving\\n3 experiences across SmartDrive co,ltd and 2 other companies\\n3 experiences across SmartDrive co,ltd and 2 other companies', 'SNS marketer\\nSNS marketer\\nSeattle Cherry Blossom and Japanese Cultural Festival\\nSeattle Cherry Blossom and Japanese Cultural Festival\\nJan 2019 - Apr 2019 · 4 mos\\nJan 2019 - Apr 2019 · 4 mos\\nSeattle\\nSeattle\\n・Community service volunteer at Japanese cultural based non-profit organization.\\n・Increased the highest post reach from 1560 to 3800.\\n・Increased the highest post engagement from 109 to 570.\\nLog In or Sign Up to View\\nLog In or Sign Up to View\\nSee posts, photos and more on Facebook.\\nSee posts, photos and more on Facebook.', '19 search appearances\\n19 search appearances\\nSee how often you appear in search results.\\nSee how often you appear in search results.', 'Creator mode\\nCreator mode\\nOff\\nOff\\nGet discovered, showcase content on your profile, and get access to creator tools\\nGet discovered, showcase content on your profile, and get access to creator tools', 'Ravinuthala Kousalya, MEM\\nRavinuthala Kousalya, MEM\\nSr Business Analyst@ Collins Aerospace | MEM graduate @ Purdue University- Class of 2022\\nConnect', 'Lavanya Swaminathan\\nLavanya Swaminathan\\nWomen in Engineering - Mentees and Mentors Leadership Team Member | Aeronautical and Astronautical Engineering student at Purdue University\\nConnect', 'Microsoft\\nMicrosoft\\n19,248,833 followers\\n19,248,833 followers\\nFollowing', 'Microsoft Office Specialist Excel 2016\\nMicrosoft Office Specialist Excel 2016\\nMicrosoft\\nMicrosoft\\nIssued May 2019\\nIssued May 2019\\nShow credential', 'Team Leadership\\nTeam Leadership', 'Digital Marketing Intern\\nDigital Marketing Intern\\nNet-Protection\\nNet-Protection\\nJan 2018 - May 2018 · 5 mos\\nJan 2018 - May 2018 · 5 mos\\nGinza, Tokyo Japan\\nGinza, Tokyo Japan\\n・Managed and improved the company homepage and landing pages. \\n・Also administered the Facebook group for the class of 2021.\\n・Managed and improved the company homepage and landing pages. ・Also administered the Facebook group for the class of 2021.\\nSkills: Search Engine Optimization (SEO) · Analytical Skills\\nSearch Engine Optimization (SEO) · Analytical Skills', \"93 post impressions\\n93 post impressions\\nCheck out who's engaging with your posts.\\nCheck out who's engaging with your posts.\", '阿部悠大\\n阿部悠大\\n· 2nd\\nSecond degree connection\\nFujitsu - CEO Office \\nFujitsu - CEO Office\\nConnect']\n"
     ]
    }
   ],
   "source": [
    "# get current URL source code\n",
    "# https://bit.ly/3JZGyR2\n",
    "pages = driver.page_source\n",
    "\n",
    "# use beautiful Soup to get access tags\n",
    "## warning: beautiful soup uses Xpath\n",
    "linkedin_soup = bs(pages.encode(\"utf-8\"), \"html\")\n",
    "linkedin_soup.prettify()\n",
    "# containers = linkedin_soup.find_all(\"div\", {\"class\":\"pvs-entity pvs-entity--padded pvs-list__item--no-padding-in-columns\"})\n",
    "# containers = linkedin_soup.find_all(\"div\", {\"class\":\"display-flex flex-column full-width align-self-center\"})\n",
    "\n",
    "time.sleep(5)\n",
    "find = driver.find_elements(by = By.XPATH, value = '//div[@class=\"display-flex flex-column full-width align-self-center\"]')\n",
    "\n",
    "l = []\n",
    "for i in range(len(find)):\n",
    "    l.append(find[i].text)\n",
    "\n",
    "list = [*set(l)]\n",
    "print(list)\n",
    "# containers = linkedin_soup.find_all(\"div\", {\"class\":\"pvs-entity pvs-entitypvs-entity--padded pvs-list__item--no-padding-in-columns\"})\n",
    "#containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(os.path.join(self.linkedin_url, \"people\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from .objects import Scraper\n",
    "from .person import Person\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "AD_BANNER_CLASSNAME = ('ad-banner-container', '__ad')\n",
    "\n",
    "def getchildren(elem):\n",
    "    return elem.find_elements(By.XPATH, \".//*\")\n",
    "\n",
    "class CompanySummary(object):\n",
    "    linkedin_url = None\n",
    "    name = None\n",
    "    followers = None\n",
    "\n",
    "    def __init__(self, linkedin_url = None, name = None, followers = None):\n",
    "        self.linkedin_url = linkedin_url\n",
    "        self.name = name\n",
    "        self.followers = followers\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.followers == None:\n",
    "            return \"\"\" {name} \"\"\".format(name = self.name)\n",
    "        else:\n",
    "            return \"\"\" {name} {followers} \"\"\".format(name = self.name, followers = self.followers)\n",
    "\n",
    "class Company(Scraper):\n",
    "    linkedin_url = None\n",
    "    name = None\n",
    "    about_us =None\n",
    "    website = None\n",
    "    headquarters = None\n",
    "    founded = None\n",
    "    industry = None\n",
    "    company_type = None\n",
    "    company_size = None\n",
    "    specialties = None\n",
    "    showcase_pages = []\n",
    "    affiliated_companies = []\n",
    "    employees = []\n",
    "    headcount = None\n",
    "\n",
    "    def __init__(self, linkedin_url = None, name = None, about_us =None, website = None, headquarters = None, founded = None, industry = None, company_type = None, company_size = None, specialties = None, showcase_pages =[], affiliated_companies = [], driver = None, scrape = True, get_employees = True, close_on_complete = True):\n",
    "        self.linkedin_url = linkedin_url\n",
    "        self.name = name\n",
    "        self.about_us = about_us\n",
    "        self.website = website\n",
    "        self.headquarters = headquarters\n",
    "        self.founded = founded\n",
    "        self.industry = industry\n",
    "        self.company_type = company_type\n",
    "        self.company_size = company_size\n",
    "        self.specialties = specialties\n",
    "        self.showcase_pages = showcase_pages\n",
    "        self.affiliated_companies = affiliated_companies\n",
    "\n",
    "        if driver is None:\n",
    "            try:\n",
    "                if os.getenv(\"CHROMEDRIVER\") == None:\n",
    "                    driver_path = os.path.join(os.path.dirname(__file__), 'drivers/chromedriver')\n",
    "                else:\n",
    "                    driver_path = os.getenv(\"CHROMEDRIVER\")\n",
    "\n",
    "                driver = webdriver.Chrome(driver_path)\n",
    "            except:\n",
    "                driver = webdriver.Chrome()\n",
    "\n",
    "        driver.get(linkedin_url)\n",
    "        self.driver = driver\n",
    "\n",
    "        if scrape:\n",
    "            self.scrape(get_employees=get_employees, close_on_complete=close_on_complete)\n",
    "\n",
    "    def __get_text_under_subtitle(self, elem):\n",
    "        return \"\\n\".join(elem.text.split(\"\\n\")[1:])\n",
    "\n",
    "    def __get_text_under_subtitle_by_class(self, driver, class_name):\n",
    "        return self.__get_text_under_subtitle(driver.find_element(By.CLASS_NAME, class_name))\n",
    "\n",
    "    def scrape(self, get_employees=True, close_on_complete=True):\n",
    "        if self.is_signed_in():\n",
    "            self.scrape_logged_in(get_employees = get_employees, close_on_complete = close_on_complete)\n",
    "        else:\n",
    "            self.scrape_not_logged_in(get_employees = get_employees, close_on_complete = close_on_complete)\n",
    "\n",
    "    def __parse_employee__(self, employee_raw):\n",
    "\n",
    "        try:\n",
    "            # print()\n",
    "            employee_object = {}\n",
    "            employee_object['name'] = (employee_raw.text.split(\"\\n\") or [\"\"])[0].strip()\n",
    "            employee_object['designation'] = (employee_raw.text.split(\"\\n\") or [\"\"])[3].strip()\n",
    "            employee_object['linkedin_url'] = employee_raw.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "            # print(employee_raw.text, employee_object)\n",
    "            # _person = Person(\n",
    "            #     # linkedin_url = employee_raw.find_element_by_tag_name(\"a\").get_attribute(\"href\"),\n",
    "            #     linkedin_url = employee_raw.find_element_by_tag_name(\"a\").get_attribute(\"href\"),\n",
    "            #     name = (employee_raw.text.split(\"\\n\") or [\"\"])[0].strip(),\n",
    "            #     driver = self.driver,\n",
    "            #     get = True,\n",
    "            #     scrape = False,\n",
    "            #     designation = (employee_raw.text.split(\"\\n\") or [\"\"])[3].strip()\n",
    "            #     )\n",
    "            # print(_person, employee_object)\n",
    "            # return _person\n",
    "            return employee_object\n",
    "        except Exception as e:\n",
    "            # print(e)\n",
    "            return None\n",
    "\n",
    "    def get_employees(self, wait_time=10):\n",
    "        total = []\n",
    "        list_css = \"list-style-none\"\n",
    "        next_xpath = '//button[@aria-label=\"Next\"]'\n",
    "        driver = self.driver\n",
    "\n",
    "        try:\n",
    "            see_all_employees = driver.find_element(By.XPATH,'//a[@data-control-name=\"topcard_see_all_employees\"]')\n",
    "        except:\n",
    "            pass\n",
    "        driver.get(os.path.join(self.linkedin_url, \"people\"))\n",
    "\n",
    "        _ = WebDriverWait(driver, 3).until(EC.presence_of_all_elements_located((By.XPATH, '//span[@dir=\"ltr\"]')))\n",
    "\n",
    "        driver.execute_script(\"window.scrollTo(0, Math.ceil(document.body.scrollHeight/2));\")\n",
    "        time.sleep(1)\n",
    "        driver.execute_script(\"window.scrollTo(0, Math.ceil(document.body.scrollHeight*3/4));\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        results_list = driver.find_element(By.CLASS_NAME, list_css)\n",
    "        results_li = results_list.find_elements(By.TAG_NAME, \"li\")\n",
    "        for res in results_li:\n",
    "            total.append(self.__parse_employee__(res))\n",
    "\n",
    "        def is_loaded(previous_results):\n",
    "          loop = 0\n",
    "          driver.execute_script(\"window.scrollTo(0, Math.ceil(document.body.scrollHeight));\")\n",
    "          results_li = results_list.find_elements(By.TAG_NAME, \"li\")\n",
    "          while len(results_li) == previous_results and loop <= 5:\n",
    "            time.sleep(1)\n",
    "            driver.execute_script(\"window.scrollTo(0, Math.ceil(document.body.scrollHeight));\")\n",
    "            results_li = results_list.find_elements(By.TAG_NAME, \"li\")\n",
    "            loop += 1\n",
    "          return loop <= 5\n",
    "\n",
    "        def get_data(previous_results):\n",
    "            results_li = results_list.find_elements(By.TAG_NAME, \"li\")\n",
    "            for res in results_li[previous_results:]:\n",
    "                total.append(self.__parse_employee__(res))\n",
    "\n",
    "        results_li_len = len(results_li)\n",
    "        while is_loaded(results_li_len):\n",
    "            try:\n",
    "                driver.find_element(By.XPATH,next_xpath).click()\n",
    "            except:\n",
    "                pass\n",
    "            _ = WebDriverWait(driver, wait_time).until(EC.presence_of_element_located((By.CLASS_NAME, list_css)))\n",
    "\n",
    "            driver.execute_script(\"window.scrollTo(0, Math.ceil(document.body.scrollHeight/2));\")\n",
    "            time.sleep(1)\n",
    "            driver.execute_script(\"window.scrollTo(0, Math.ceil(document.body.scrollHeight*2/3));\")\n",
    "            time.sleep(1)\n",
    "            driver.execute_script(\"window.scrollTo(0, Math.ceil(document.body.scrollHeight*3/4));\")\n",
    "            time.sleep(1)\n",
    "            driver.execute_script(\"window.scrollTo(0, Math.ceil(document.body.scrollHeight));\")\n",
    "            time.sleep(1)\n",
    "\n",
    "            get_data(results_li_len)\n",
    "            results_li_len = len(total)\n",
    "        return total\n",
    "\n",
    "\n",
    "\n",
    "    def scrape_logged_in(self, get_employees = True, close_on_complete = True):\n",
    "        driver = self.driver\n",
    "\n",
    "        driver.get(self.linkedin_url)\n",
    "\n",
    "        _ = WebDriverWait(driver, 3).until(EC.presence_of_all_elements_located((By.XPATH, '//span[@dir=\"ltr\"]')))\n",
    "\n",
    "        navigation = driver.find_element(By.CLASS_NAME, \"org-page-navigation__items \")\n",
    "\n",
    "        self.name = driver.find_element(By.XPATH,'//span[@dir=\"ltr\"]').text.strip()\n",
    "\n",
    "        # Click About Tab or View All Link\n",
    "        try:\n",
    "          self.__find_first_available_element__(\n",
    "            navigation.find_elements(By.XPATH, \"//a[@data-control-name='page_member_main_nav_about_tab']\"),\n",
    "            navigation.find_elements(By.XPATH, \"//a[@data-control-name='org_about_module_see_all_view_link']\"),\n",
    "          ).click()\n",
    "        except:\n",
    "          driver.get(os.path.join(self.linkedin_url, \"about\"))\n",
    "\n",
    "        _ = WebDriverWait(driver, 3).until(EC.presence_of_all_elements_located((By.TAG_NAME, 'section')))\n",
    "        time.sleep(3)\n",
    "\n",
    "        if 'Cookie Policy' in driver.find_elements(By.TAG_NAME, \"section\")[1].text or any(classname in driver.find_elements(By.TAG_NAME, \"section\")[1].get_attribute('class') for classname in AD_BANNER_CLASSNAME):\n",
    "            section_id = 4\n",
    "        else:\n",
    "            section_id = 3\n",
    "       #section ID is no longer needed, we are using class name now.\n",
    "        #grid = driver.find_elements_by_tag_name(\"section\")[section_id]\n",
    "        grid = driver.find_element(By.CLASS_NAME, \"artdeco-card.p5.mb4\")\n",
    "        print(grid)\n",
    "        descWrapper = grid.find_elements(By.TAG_NAME, \"p\")\n",
    "        if len(descWrapper) > 0:\n",
    "            self.about_us = descWrapper[0].text.strip()\n",
    "        labels = grid.find_elements(By.TAG_NAME, \"dt\")\n",
    "        values = grid.find_elements(By.TAG_NAME, \"dd\")\n",
    "        num_attributes = min(len(labels), len(values))\n",
    "        #print(\"The length of the labels is \" + str(len(labels)), \"The length of the values is \" + str(len(values)))\n",
    "        # if num_attributes == 0:\n",
    "        #     exit()\n",
    "        x_off = 0\n",
    "        for i in range(num_attributes):\n",
    "            txt = labels[i].text.strip()\n",
    "            if txt == 'Website':\n",
    "                self.website = values[i+x_off].text.strip()\n",
    "            elif txt == 'Industry':\n",
    "                self.industry = values[i+x_off].text.strip()\n",
    "            elif txt == 'Company size':\n",
    "                self.company_size = values[i+x_off].text.strip()\n",
    "                if len(values) > len(labels):\n",
    "                    x_off = 1\n",
    "            elif txt == 'Headquarters':\n",
    "                    self.headquarters = values[i+x_off].text.strip()\n",
    "            elif txt == 'Type':\n",
    "                self.company_type = values[i+x_off].text.strip()\n",
    "            elif txt == 'Founded':\n",
    "                self.founded = values[i+x_off].text.strip()\n",
    "            elif txt == 'Specialties':\n",
    "                self.specialties = \"\\n\".join(values[i+x_off].text.strip().split(\", \"))\n",
    "\n",
    "        grid = driver.find_element(By.CLASS_NAME, \"mt1\")\n",
    "        spans = grid.find_elements(By.TAG_NAME, \"span\")\n",
    "        for span in spans:\n",
    "            txt = span.text.strip()\n",
    "            if \"See all\" in txt and \"employees on LinkedIn\" in txt:\n",
    "                self.headcount = int(txt.replace(\"See all\", \"\").replace(\"employees on LinkedIn\", \"\").strip())\n",
    "\n",
    "        driver.execute_script(\"window.scrollTo(0, Math.ceil(document.body.scrollHeight/2));\")\n",
    "\n",
    "\n",
    "        try:\n",
    "            _ = WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.CLASS_NAME, 'company-list')))\n",
    "            showcase, affiliated = driver.find_elements(By.CLASS_NAME, \"company-list\")\n",
    "            driver.find_element(By.ID,\"org-related-companies-module__show-more-btn\").click()\n",
    "\n",
    "            # get showcase\n",
    "            for showcase_company in showcase.find_elements(By.CLASS_NAME, \"org-company-card\"):\n",
    "                companySummary = CompanySummary(\n",
    "                        linkedin_url = showcase_company.find_element(By.CLASS_NAME, \"company-name-link\").get_attribute(\"href\"),\n",
    "                        name = showcase_company.find_element(By.CLASS_NAME, \"company-name-link\").text.strip(),\n",
    "                        followers = showcase_company.find_element(By.CLASS_NAME, \"company-followers-count\").text.strip()\n",
    "                    )\n",
    "                self.showcase_pages.append(companySummary)\n",
    "\n",
    "            # affiliated company\n",
    "\n",
    "            for affiliated_company in showcase.find_element(By.CLASS_NAME, \"org-company-card\"):\n",
    "                companySummary = CompanySummary(\n",
    "                         linkedin_url = affiliated_company.find_element(By.CLASS_NAME, \"company-name-link\").get_attribute(\"href\"),\n",
    "                        name = affiliated_company.find_element(By.CLASS_NAME, \"company-name-link\").text.strip(),\n",
    "                        followers = affiliated_company.find_element(By.CLASS_NAME, \"company-followers-count\").text.strip()\n",
    "                        )\n",
    "                self.affiliated_companies.append(companySummary)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if get_employees:\n",
    "            self.employees = self.get_employees()\n",
    "\n",
    "        driver.get(self.linkedin_url)\n",
    "\n",
    "        if close_on_complete:\n",
    "            driver.close()\n",
    "\n",
    "    def scrape_not_logged_in(self, close_on_complete = True, retry_limit = 10, get_employees = True):\n",
    "        driver = self.driver\n",
    "        retry_times = 0\n",
    "        while self.is_signed_in() and retry_times <= retry_limit:\n",
    "            page = driver.get(self.linkedin_url)\n",
    "            retry_times = retry_times + 1\n",
    "\n",
    "        self.name = driver.find_element(By.CLASS_NAME, \"name\").text.strip()\n",
    "\n",
    "        self.about_us = driver.find_element(By.CLASS_NAME, \"basic-info-description\").text.strip()\n",
    "        self.specialties = self.__get_text_under_subtitle_by_class(driver, \"specialties\")\n",
    "        self.website = self.__get_text_under_subtitle_by_class(driver, \"website\")\n",
    "        self.headquarters = driver.find_element(By.CLASS_NAME, \"adr\").text.strip()\n",
    "        self.industry = driver.find_element(By.CLASS_NAME, \"industry\").text.strip()\n",
    "        self.company_size = driver.find_element(By.CLASS_NAME, \"company-size\").text.strip()\n",
    "        self.company_type = self.__get_text_under_subtitle_by_class(driver, \"type\")\n",
    "        self.founded = self.__get_text_under_subtitle_by_class(driver, \"founded\")\n",
    "\n",
    "        # get showcase\n",
    "        try:\n",
    "            driver.find_element(By.ID,\"view-other-showcase-pages-dialog\").click()\n",
    "            WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.ID, 'dialog')))\n",
    "\n",
    "            showcase_pages = driver.find_elements(By.CLASS_NAME, \"company-showcase-pages\")[1]\n",
    "            for showcase_company in showcase_pages.find_elements(By.TAG_NAME, \"li\"):\n",
    "                name_elem = showcase_company.find_element(By.CLASS_NAME, \"name\")\n",
    "                companySummary = CompanySummary(\n",
    "                    linkedin_url = name_elem.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\"),\n",
    "                    name = name_elem.text.strip(),\n",
    "                    followers = showcase_company.text.strip().split(\"\\n\")[1]\n",
    "                )\n",
    "                self.showcase_pages.append(companySummary)\n",
    "            driver.find_element(By.CLASS_NAME, \"dialog-close\").click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # affiliated company\n",
    "        try:\n",
    "            affiliated_pages = driver.find_element(By.CLASS_NAME, \"affiliated-companies\")\n",
    "            for i, affiliated_page in enumerate(affiliated_pages.find_elements(By.CLASS_NAME, \"affiliated-company-name\")):\n",
    "                if i % 3 == 0:\n",
    "                    affiliated_pages.find_element(By.CLASS_NAME, \"carousel-control-next\").click()\n",
    "\n",
    "                companySummary = CompanySummary(\n",
    "                    linkedin_url = affiliated_page.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\"),\n",
    "                    name = affiliated_page.text.strip()\n",
    "                )\n",
    "                self.affiliated_companies.append(companySummary)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if get_employees:\n",
    "            self.employees = self.get_employees()\n",
    "\n",
    "        driver.get(self.linkedin_url)\n",
    "\n",
    "        if close_on_complete:\n",
    "            driver.close()\n",
    "\n",
    "    def __repr__(self):\n",
    "        _output = {}\n",
    "        _output['name'] = self.name\n",
    "        _output['about_us'] = self.about_us\n",
    "        _output['specialties'] = self.specialties\n",
    "        _output['website'] = self.website\n",
    "        _output['industry'] = self.industry\n",
    "        _output['company_type'] = self.name\n",
    "        _output['headquarters'] = self.headquarters\n",
    "        _output['company_size'] = self.company_size\n",
    "        _output['founded'] = self.founded\n",
    "        _output['affiliated_companies'] = self.affiliated_companies\n",
    "        _output['employees'] = self.employees\n",
    "        _output['headcount'] = self.headcount\n",
    "        \n",
    "        return json.dumps(_output).replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrape_func(a,b,c):\n",
    "    name = a[28:-1]\n",
    "    page = a\n",
    "    time.sleep(10)\n",
    "\n",
    "    driver.get(page + 'detail/recent-activity/shares/')  \n",
    "    start=time.time()\n",
    "    lastHeight = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "        newHeight = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if newHeight == lastHeight:\n",
    "            break\n",
    "        lastHeight = newHeight\n",
    "        end=time.time()\n",
    "        if round(end-start)>20:\n",
    "            break\n",
    "\n",
    "    company_page = driver.page_source   \n",
    "\n",
    "    linkedin_soup = bs(company_page.encode(\"utf-8\"), \"html\")\n",
    "    linkedin_soup.prettify()\n",
    "    containers = linkedin_soup.findAll(\"div\",{\"class\":\"occludable-update ember-view\"})\n",
    "    print(\"Fetching data from account: \"+ name)\n",
    "    iterations = 0\n",
    "    nos = int(input(\"Enter number of posts: \"))\n",
    "    for container in containers:\n",
    "\n",
    "        try:\n",
    "            text_box = container.find(\"div\",{\"class\":\"feed-shared-update-v2__description-wrapper ember-view\"})\n",
    "            text = text_box.find(\"span\",{\"dir\":\"ltr\"})\n",
    "            b.append(text.text.strip())\n",
    "            c.append(name)\n",
    "            iterations += 1\n",
    "            print(iterations)\n",
    "            \n",
    "            if(iterations==nos):\n",
    "                break\n",
    "\n",
    "        except:\n",
    "            pass \n",
    "\n",
    "n = int(input(\"Enter the number of entries: \"))\n",
    "for i in range(n):\n",
    "    post_links.append(input(\"Enter the link: \"))\n",
    "for j in range(n):\n",
    "    Scrape_func(post_links[j],post_texts,post_names)\n",
    "\n",
    "        \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "@dataclass\n",
    "class Company():\n",
    "    name: str\n",
    "    rank: int\n",
    "    revenues: float\n",
    "    revchange: float\n",
    "    profits: float\n",
    "    assets: float\n",
    "    prftchange: float\n",
    "    employees: int\n",
    "    rankchange: int\n",
    "    hqcountry: str\n",
    "    newcommer: str\n",
    "    rankdrop: str\n",
    "    rankgain: str\n",
    "    sector: str\n",
    "    industry: str\n",
    "    hqcity: str\n",
    "    hqstate: str\n",
    "    profitable: str\n",
    "    ceowoman: str\n",
    "    jobgrowth: str\n",
    "    fortune500: str\n",
    "    title: str\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, jstring):\n",
    "        c = {}\n",
    "        for i in jstring:\n",
    "            c[i['key'].replace('-', '_').lower()] = \\\n",
    "                    i['value'].replace('&#038;', '&').replace('&#8217;', \"'\")\n",
    "\n",
    "        #print(f\"Rank: {c['rank']:4}  Name: {c['name']}\")\n",
    "        #print(json.dumps(c, indent=4))\n",
    "        return cls(c['name'], int(c['rank']), float(c['revenues']), \n",
    "                   float(c['revchange']), float(c['profits']), float(c['assets']), \n",
    "                   c['prftchange'], int(c['employees']), c['rankchange'], \n",
    "                   c['hqcountry'], c['newcomer'], c['rankdrop'], c['rankgain'],\n",
    "                   c['sector'], c['industry'], c['hqcity'], c['hqstate'],\n",
    "                   c['profitable'], c['ceowoman'], c['jobgrowth'],\n",
    "                   c['fortune500_y_n'], c['title'])\n",
    "\n",
    "\n",
    "def fortune500_list():\n",
    "    url = 'https://content.fortune.com/wp-json/irving/v1/data/franchise-search-results?list_id=2666483'\n",
    "    r = requests.get(url)\n",
    "    #r.encoding = 'ISO-8859-1'\n",
    "    json_data = json.loads(r.text)[1]['items']\n",
    "    companies = []\n",
    "    for comp in json_data:\n",
    "        companies.append(Company.from_json(comp['fields']))\n",
    "    return companies\n",
    "\n",
    "\n",
    "f500 = fortune500_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c919f66377084bbe5264acaad3227a0b6e2b7b31c717b6c23ee3b90ae87f6f0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
